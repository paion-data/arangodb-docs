<!doctype html><html lang=en><head><link href=//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.no-icons.min.css rel=stylesheet><link href=//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css rel=stylesheet><link href=/css/fontawesome-all.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css rel=stylesheet></noscript><link href=/css/featherlight.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/featherlight.min.css rel=stylesheet></noscript><link href=/css/nucleus.css rel=stylesheet><link href=/css/fonts.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fonts.css rel=stylesheet></noscript><link href=/css/theme.css rel=stylesheet><link href=/css/theme-relearn-light.css rel=stylesheet id=variant-style><link href=/css/print.css rel=stylesheet media=print><script src=/js/variant.js?1743774193></script>
<script>var root_url="/",baseUriFull,baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="",window.T_Copied_to_clipboard="",window.T_Copy_link_to_clipboard="",window.T_Link_copied_to_clipboard="",baseUriFull="http://localhost/",window.variants&&variants.init(["relearn-light"])</script><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.119.0"><meta itemprop=description property="description" content="ArangoDB is a scalable graph database system with native support for other data models and a built-in search engine, for the cloud and on-premises"><meta property="og:url" content="http://localhost/3.11/operations/administration/reduce-memory-footprint/"><meta property="og:title" content="Reducing the Memory Footprint of ArangoDB servers"><meta property="og:type" content="website"><meta property="og:description" content="ArangoDB is a scalable graph database system with native support for other data models and a built-in search engine, for the cloud and on-premises"><meta name=docsearch:version content="3.11"><title>Reducing the Memory Footprint of ArangoDB servers | ArangoDB Documentation</title><link href=/images/favicon.png rel=icon type=image/png><script src=/js/jquery.min.js></script>
<script src=/js/clipboard.min.js?1743774193 defer></script>
<script src=/js/featherlight.min.js?1743774193 defer></script>
<script>var versions=[{alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"},{alias:"stable",deprecated:!1,name:"3.12",version:"3.12.4"},{alias:"3.11",deprecated:!1,name:"3.11",version:"3.11.13"},{alias:"3.10",deprecated:!0,name:"3.10",version:"3.10.14"}]</script><script>var develVersion={alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"}</script><script>var stableVersion={alias:"stable",deprecated:!1,name:"3.12",version:"3.12.4"}</script><script src=/js/codeblocks.js?1743774193 defer></script>
<script src=/js/theme.js?1743774193 defer></script></head><body><noscript>You need to enable JavaScript to use the ArangoDB documentation.</noscript><div id=page-wrapper class=page_content_splash style=height:auto;opacity:0><section id=page-main><section class=page-container id=page-container><header id=header style="transition:.5s padding ease-out,.15s" class="zn_header_white header-splash-new nav-down header-splash-wrap header1"><div class=header-block-left><div class=mobile-menu-toggle><button id=sidebar-toggle-navigation onclick=showSidebarHandler()><svg width="1.33em" height="1.33em" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button></div><div class=version-logo-container><div class="logo-container hasinfocard_img arangodb-logo-large"><div class=logo><a href=https://www.arangodb.com/><img src=/images/logo_main.png alt=ArangoDB title></a></div></div><div class=arangodb-logo-small><a href=https://arangodb.com/><img alt="ArangoDB Logo" src=/images/ArangoDB_Logo_White_small.png></a></div></div></div><div class=container-right style=display:hidden></div><div class=search-and-version-container><a href=# class="home-link is-current" aria-label="Go to home page" onclick=goToHomepage(event)></a><div id=searchbox></div><script type=text/javascript>const SCRIPT_SRC="https://unpkg.com/@inkeep/widgets-embed@0.2.290/dist/embed.js";function loadAndInitializeInkeep(){if(document.querySelector(`script[src="${SCRIPT_SRC}]"`))return;const e=document.createElement("script");e.type="module",e.src=SCRIPT_SRC,e.onload=initializeInkeep,document.head.appendChild(e)}function initializeInkeep(){const e=Inkeep({integrationId:"clo4lx6jk0000s601cp21x2ok",apiKey:"13b4e56966a76e86c6ff359cd795ee6a0412f751d75d6383",organizationId:"org_HGBkkzGAa4KeGJGh",organizationDisplayName:"ArangoDB",primaryBrandColor:"#80a54d",stringReplacementRules:[{matchingRule:{ruleType:"Substring",string:"Arangograph"},replaceWith:"ArangoGraph"},{matchingRule:{ruleType:"Substring",string:"Aql"},replaceWith:"AQL"},{matchingRule:{ruleType:"Substring",string:"Arangodb"},replaceWith:"ArangoDB"}],customCardSettings:[{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arango.qubitpi.org"}},searchTabLabel:"Official Docs"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"developer.arangodb.com"}},searchTabLabel:"Developer Hub"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arangodb.com"}},searchTabLabel:"Home"}]}),t=e.embed({componentType:"ChatButton",properties:{stylesheetUrls:["/css/fonts.css"],fixedPositionXOffset:"52px",baseSettings:{theme:{primaryColors:{textColorOnPrimary:"white"},tokens:{fonts:{body:"'Inter'",heading:"'Inter'"},zIndex:{overlay:1e4,modal:11e3,popover:12e3,skipLink:13e3,toast:14e3,tooltip:15e3}}}},aiChatSettings:{botAvatarSrcUrl:"/images/ArangoDB_Logo_White_small.png",quickQuestions:["What can you do with AQL that is not feasible with SQL?","How do I search for objects within arrays?","Where can I deploy my ArangoDB instance?"],getHelpCallToActions:[{icon:{builtIn:"FaSlack"},name:"Slack",url:"https://arangodb-community.slack.com/"}]},searchSettings:{tabSettings:{isAllTabEnabled:!1,alwaysDisplayedTabs:["Official Docs","Developer Hub","Home"]}}}})}loadAndInitializeInkeep()</script><div class=version-selector><select id=arangodb-version onchange=changeVersion()><option value=3.13>3.13</option><option value=3.12>3.12</option><option value=3.11>3.11</option><option value=3.10>3.10</option><option value=3.9>3.9</option><option value=3.8>3.8</option></select></div></div></header><iframe src=/nav.html title=description id=menu-iframe class="menu-iframe active" style=opacity:0></iframe><div class=container-main><div class=row-main><nav id=breadcrumbs><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><meta itemprop=itemListOrder content="Descending"><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="3.11"><a itemprop=item class=link href=/3.11/><span itemprop=name class=breadcrumb-entry>3.11.13</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Operations"><a itemprop=item class=link href=/3.11/operations/><span itemprop=name class=breadcrumb-entry>Operations</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Administration"><a itemprop=item class=link href=/3.11/operations/administration/><span itemprop=name class=breadcrumb-entry>Administration</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Reduce Memory Footprint"><a itemprop=item class=link href=/3.11/operations/administration/reduce-memory-footprint/><span itemprop=name class=breadcrumb-entry>Reduce Memory Footprint</span></a></li></ol></nav><article class=default><hgroup><h1>Reducing the Memory Footprint of ArangoDB servers</h1></hgroup><div class="box notices cstyle warning"><div class=box-content-container><div class=box-content><i class="fas fa-exclamation-triangle"></i><div class=box-text>The changes suggested here can be useful to reduce the memory usage of
ArangoDB servers, but they can cause side-effects on performance and other
aspects.
Do not apply any of the changes suggested here before you have tested them in
in a development or staging environment.</div></div></div></div><p>Usually, a database server tries to use all the memory it can get to
improve performance by caching or buffering. Therefore, it is important
to tell an ArangoDB process how much memory it is allowed to use.
ArangoDB detects the available RAM on the server and divides this up
amongst its subsystems in some default way, which is suitable for a wide
range of applications. This detected RAM size can be overridden with
the environment variable
<a href=/3.11/components/arangodb-server/environment-variables/ class=link><code>ARANGODB_OVERRIDE_DETECTED_TOTAL_MEMORY</code></a>.</p><p>However, there may be situations in which there
are good reasons why these defaults are not suitable and the
administrator wants to fine tune memory usage. The two
major reasons for this are:</p><ul><li>something else (potentially another <code>arangod</code> server) is running on
the same machine so that your <code>arangod</code> is supposed to use less than
the available RAM, and/or</li><li>the actual usage scenario makes it necessary to increase the memory
allotted to some subsystem at the cost of another to achieve higher
performance for specific tasks.</li></ul><p>To a lesser extent, the same holds true for CPU usage, but operating
systems are generally better in automatically distributing available CPU
capacity amongst different processes and different subsystems.</p><p>There are settings to make ArangoDB run on systems with very
limited resources, but they may also be interesting for your
development machine if you want to make it less taxing for
the hardware and do not work with much data. For production
environments, it is recommended to use less restrictive settings, to
<a href=https://www.arangodb.com/performance/ target=_blank rel="noopener noreferrer" class=link>benchmark</a>&nbsp;<i class="fas fa-external-link-alt"></i> your setup and
fine-tune the settings for maximal performance.</p><h2 id=limiting-the-overall-ram-usage>Limiting the overall RAM usage <a href=/3.11/operations/administration/reduce-memory-footprint/#limiting-the-overall-ram-usage class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>A first simple approach could be to simply tell the <code>arangod</code> process
as a whole to use only a certain amount of memory. This is done by
overriding the detected memory size using the
<a href=/3.11/components/arangodb-server/environment-variables/ class=link><code>ARANGODB_OVERRIDE_DETECTED_TOTAL_MEMORY</code></a>
environment variable.</p><p>This essentially scales down <code>arangod</code>&rsquo;s memory usage to the
given value. This is for example a first measure if more than
one <code>arangod</code> server are running on the same machine.</p><p>Note, however, that certain subsystems will then still be able to use
an arbitrary amount of RAM, depending on the load from the user. If you
want to protect your server against such misusages and for more detailed
tuning of the various subsystems, consult the sections below.</p><p>Before getting into the nitty-gritty details, check the overview over
the different subsystems of ArangoDB that are using significant amounts of RAM.</p><h2 id=overview-over-ram-usage-in-arangodb>Overview over RAM usage in ArangoDB <a href=/3.11/operations/administration/reduce-memory-footprint/#overview-over-ram-usage-in-arangodb class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>This section explains the various areas which use RAM and what they are
for.</p><p>Broadly, ArangoDB uses significant amounts of RAM for the following subsystems:</p><ul><li>Storage engine including the block cache</li><li>HTTP server and queues</li><li>Edge caches and other caches</li><li>AQL queries</li><li>V8 (JavaScript features)</li><li>ArangoSearch</li><li>AgencyCache/ClusterInfo (cluster meta data)</li><li>Cluster-internal replication</li></ul><p>Of these, the storage engine itself has some subsystems which contribute
towards its memory usage:</p><ul><li>RocksDB block cache</li><li>Data structures to read data (Bloom filters, index blocks, table readers)</li><li>Data structures to write data (write buffers, table builders, transaction data)</li><li>RocksDB background compaction</li></ul><p>It is important to understand that all these have a high impact on
performance. For example, the block cache is there such that already
used blocks can be retained in RAM for later access. The larger the
cache, the more blocks can be retained and so the higher the probability
that a subsequent access does not need to reach out to disk and thus
incurs no additional cost on performance and IO capacity.</p><p>RocksDB can cache bloom filters and index blocks of its SST files in RAM.
It usually needs approx. 1% of the total size of all SST files in RAM
just for caching these. As a result, most random accesses need only a
single actual disk read to find the data stored under a single RocksDB
key! If not all Bloom filters and block indexes can be held in RAM, then
this can easily slow down random accesses by a factor of 10 to 100,
since suddenly many disk reads are needed to find a single
RocksDB key!</p><p>Other data structures for reading (table readers) and writing (write
batches and write buffers) are also needed for smooth operation. If one
cuts down on these too harshly, reads can be slowed down considerably
and writes can be delayed. The latter can lead to overall slowdowns and
could even end up in total write stalls and stops until enough write
buffers have been written out to disk.</p><p>Since RocksDB is a log structured merge tree (LSM), there must be
background threads to perform compactions. They merge different SST
files and throw out old, no-longer used data. These compaction
operations also need considerable amounts of RAM. If this is limited too
much (by reducing the number of concurrent compaction operations), then
a compaction debt can occur, which also results in write stalls or
stops.</p><p>The other subsystems outside the storage engine also need RAM. If an
ArangoDB server queues up too many requests (for example, if more
requests arrive per time than can be executed), then the data of these
requests (headers, bodies, etc.) are stored in RAM until they can be
processed.</p><p>Furthermore, there are multiple different caches which are sitting in
front of the storage engine, the most prominent one being the edge cache
which helps to speed up graph traversals. The larger these caches,
the more data can be cached and the higher the likelihood that data
which is needed repeatedly is found to be available in cache.</p><p>Essentially, all AQL queries are executed in RAM. That means that every
single AQL query needs some RAM - both on Coordinators and on DB-Servers.
It is possible to limit the memory usage of a single AQL query as well
as the global usage for all AQL queries running concurrently. Obviously,
if either of these limits is reached, an AQL query can fail due to a lack
of RAM, which is then reported back to the user.</p><p>Everything which executes JavaScript (only on Coordinators, user defined
AQL functions and Foxx services), needs RAM, too. If JavaScript is not
to be used, memory can be saved by reducing the number of V8 contexts.</p><p>ArangoSearch uses memory in different ways:</p><ul><li>Writes which are committed in RocksDB but have not yet been <strong>committed</strong> to
the search index are buffered in RAM,</li><li>The search indexes use memory for <strong>consolidation</strong> of multiple smaller
search index segments into fewer larger ones,</li><li>The actual indexed search data resides in memory mapped files, which
also need RAM to be cached.</li></ul><p>Finally, the cluster internal management uses RAM in each <code>arangod</code>
instance. The whole meta data of the cluster is kept in the AgencyCache
and in the ClusterInfo cache. There is very little one can change about
this memory usage.</p><p>Furthermore, the cluster-internal replication needs memory to perform
its synchronization and replication work. Again, there is not a lot one
can do about that.</p><p>The following sections show the various subsystems which the
administrator can influence and explain how this can be done.</p><h2 id=write-ahead-log-wal>Write ahead log (WAL) <a href=/3.11/operations/administration/reduce-memory-footprint/#write-ahead-log-wal class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>RocksDB writes all changes first to the write ahead log (WAL). This is
for crash recovery, the data will then later be written in an orderly
fashion to disk. The WAL itself does not need a lot of RAM, but limiting
its total size can lead to the fact that write buffers are flushed
earlier to make older WAL files obsolete. Therefore, adjusting the
option</p><pre tabindex=0><code>--rocksdb.max-total-wal-size
</code></pre><p>to some value smaller than its default of 80MB can potentially help
to reduce RAM usage. However, the effect is rather indirect.</p><h2 id=write-buffers>Write Buffers <a href=/3.11/operations/administration/reduce-memory-footprint/#write-buffers class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>RocksDB writes into
<a href=https://github.com/facebook/rocksdb/wiki/Write-Buffer-Manager target=_blank rel="noopener noreferrer" class=link>memory buffers mapped to on-disk blocks</a>&nbsp;<i class="fas fa-external-link-alt"></i>
first. At some point, the memory buffers will be full and have to be
flushed to disk. In order to support high write loads, RocksDB might
open a lot of these memory buffers.</p><p>By default, the system may use up to 10 write buffers per column family
and the default write buffer size is 64MB. Since normal write loads will
only hit the documents, edge, primary index and vpack index column
families, this effectively limits the RAM usage to something like 2.5GB.
However, there is a global limit which works across column families,
which can be used to limit the total amount of memory for write buffers
(set to unlimited by default!):</p><pre tabindex=0><code>--rocksdb.total-write-buffer-size
</code></pre><p>Note that it does not make sense to set this limit smaller than
10 times the size of a write buffer, since there are currently 10 column
families and each will need at least one write buffer.</p><p>Additionally, RocksDB might keep some write buffers in RAM, which are
already flushed to disk. This is to speed up transaction conflict
detection. This only happens if the option</p><pre tabindex=0><code>--rocksdb.max-write-buffer-size-to-maintain
</code></pre><p>is set to a non-zero value. By default, this is set to 0.</p><p>The other options to configure write buffer usage are:</p><pre tabindex=0><code>--rocksdb.write-buffer-size
--rocksdb.max-write-buffer-number
--rocksdb.max-write-buffer-number-definitions
--rocksdb.max-write-buffer-number-documents
--rocksdb.max-write-buffer-number-edge
--rocksdb.max-write-buffer-number-fulltext
--rocksdb.max-write-buffer-number-geo
--rocksdb.max-write-buffer-number-primary
--rocksdb.max-write-buffer-number-replicated-logs
--rocksdb.max-write-buffer-number-vpack
</code></pre><p>However, adjusting these usually not helps with RAM usage.</p><h2 id=rocksdb-block-cache>RocksDB Block Cache <a href=/3.11/operations/administration/reduce-memory-footprint/#rocksdb-block-cache class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The RocksDB block cache has a number of options for configuration. First
of all, its maximal size can be adjusted with the following option:</p><pre tabindex=0><code>--rocksdb.block-cache-size
</code></pre><p>The default is 30% of (R - 2GB) where R is the total detected RAM. This
is a sensible default for many cases, but in particular if other
services are running on the same system, this can be too large. On the
other hand, if lots of other things are accounted with the block cache
(see options below), the value can also be too small.</p><p>Sometimes, the system can temporarily use a bit more than the configured
upper limit. If you want to strictly enforce the block cache size limit,
you can set the option</p><pre tabindex=0><code>--rocksdb.enforce-block-cache-size-limit
</code></pre><p>to <code>true</code>, but it is not recommended since it might lead to failed
operations if the cache is full and we have observed that RocksDB
instances can get stuck in this case. You have been warned.</p><p>There are a number of things for which RocksDB needs RAM. It is possible
to make it so that all of these RAM usages count towards the block cache
usage. This is usually sensible, since it effectively allows to keep
RocksDB RAM usage under a certain configured limit (namely the block
cache size limit). If these things are not accounted in the block cache
usage, they are allocated anyway and this can lead to too much memory
usage. On the other hand, if they are accounted in the block cache
usage, then the block cache has less capacity for its core operations,
the caching of data blocks.</p><p>The following options control accounting for RocksDB RAM usage:</p><pre tabindex=0><code>--rocksdb.cache-index-and-filter-blocks
--rocksdb.cache-index-and-filter-blocks-with-high-priority
--rocksdb.reserve-file-metadata-memory
--rocksdb.reserve-table-builder-memory
--rocksdb.reserve-table-reader-memory
</code></pre><p>They are for Bloom filter and block indexes, file metadata, table
building (RAM usage for building SST files, this happens when flushing
memtables to level 0 SST files, during compaction and on recovery)
and table reading (RAM usage for read operations) respectively.</p><p>There are additional options you can enable to avoid that the index and filter
blocks get evicted from cache:</p><pre tabindex=0><code>--rocksdb.pin-l0-filter-and-index-blocks-in-cache
--rocksdb.pin-top-level-index-and-filter
</code></pre><p>The first does level 0 Bloom filters and index blocks and its default is
<code>false</code>. The second does top level of partitioned index blocks and Bloom
filters into the cache, its default is <code>true</code>.</p><p>The block cache basically trades increased RAM usage for less disk I/O, so its
size does not only affect memory usage, but can also affect read performance.</p><p>See also:</p><ul><li><a href=/3.11/components/arangodb-server/options/#rocksdb class=link>RocksDB Server Options</a></li><li><a href=https://github.com/facebook/rocksdb/wiki/Write-Buffer-Manager target=_blank rel="noopener noreferrer" class=link>Write Buffer Manager</a>&nbsp;<i class="fas fa-external-link-alt"></i></li></ul><h2 id=transactions>Transactions <a href=/3.11/operations/administration/reduce-memory-footprint/#transactions class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Before commit, RocksDB builds up transaction data in RAM. This happens
in so-called &ldquo;memtables&rdquo; and &ldquo;write batches&rdquo;. Note that from
v3.12 onwards, this memory usage is accounted for in writing AQL queries and
other write operations, and can thus be limited there. When there are
many or large open transactions, this can sum up to a large amount of
RAM usage.</p><p>A further limit on RAM usage can be imposed by setting the option</p><pre tabindex=0><code>--rocksdb.max-transaction-size
</code></pre><p>which is by default unlimited. This setting limits the total size of
a single RocksDB transaction. If the memtables exceed this size,
the transaction is automatically aborted. Note that this cannot guard
against <strong>many</strong> simultaneously uncommitted transactions.</p><p>Another way to limit actual transaction size is &ldquo;intermediate commits&rdquo;.
This is a setting which leads to the behavior that ArangoDB
automatically commits large write operations while they are executed. This of
course goes against the whole concept of a transaction, since parts of a
transaction which have already been committed cannot be rolled back any
more. Therefore, this is a rather desperate measure to prevent RAM
overusage. You can control this with the following options:</p><pre tabindex=0><code>--rocksdb.intermediate-commit-count
--rocksdb.intermediate-commit-size
</code></pre><p>The first option configures automatic intermediate commits based on the number
of documents touched in the transaction (default is <code>1000000</code>). The second
configures intermediate commits based on the total size of the documents
touched in the transaction (default is <code>512MB</code>).</p><h2 id=rocksdb-compactions>RocksDB compactions <a href=/3.11/operations/administration/reduce-memory-footprint/#rocksdb-compactions class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>RocksDB compactions are necessary, but use RAM. You can control how many
concurrent compactions can happen by configuring the number of
background threads RocksDB can use. This can either be done with the</p><pre tabindex=0><code>--rocksdb.max-background-jobs
</code></pre><p>option whose default is the number of detected cores. Half
of that number will be the default value for the option</p><pre tabindex=0><code>--rocksdb.num-threads-priority-low
</code></pre><p>and that many compactions can happen concurrently. You can also leave
the total number of background jobs and just adjust the latter option.
Fewer concurrent compaction jobs will use less RAM, but will also lead
to slower compaction overall, which can lead to write stalls and even
stops, if a compaction debt builds up under a high write load.</p><h2 id=scheduler-queues>Scheduler queues <a href=/3.11/operations/administration/reduce-memory-footprint/#scheduler-queues class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>If too much memory is used to queue requests in the scheduler queues,
one can simply limit the queue length with this option:</p><pre tabindex=0><code>--server.scheduler-queue-size
</code></pre><p>The default is <code>4096</code>, which is quite a lot. For small requests, the
memory usage for a full queue will not be significant, but since
individual requests can be large, it may sometimes be necessary to
limit the queue size a lot more to avoid RAM over usage on the queue.</p><h2 id=index-caches>Index Caches <a href=/3.11/operations/administration/reduce-memory-footprint/#index-caches class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>There are in-RAM caches for edge indexes and other indexes. The total
RAM limit for them can be configured with this option:</p><pre tabindex=0><code>--cache.size
</code></pre><p>By default, this is set to 25% of (R - 2GB) where R is the total
detected available RAM (or 256MB if that total is at most 4GB).
You can disable the in-memory index
<a href=/3.11/components/arangodb-server/options/#cache class=link>caches</a>, by setting the limit to 0.</p><p>If you do not have a graph use case and do not use edge collections,
nor the optional hash cache for persistent indexes, it is possible to
use no cache (or a minimal cache size) without a performance impact. In
general, this should correspond to the size of the hot-set of edges and
cached lookups from persistent indexes.</p><p>Starting with v3.11, another way to save memory in the caches is to use compression
by setting the following option:</p><pre tabindex=0><code>--cache.min-value-size-for-edge-compression
</code></pre><p>The default is <code>1GB</code> which essentially switches the compression off.
If you set this to something like 100, values with at least 100 bytes
will be compressed.</p><p>This costs a little CPU power but the system can then cache more
data in the same amount of memory.</p><p>The behavior of the hash tables used internally by the caches can be
adjusted with the following two options:</p><pre tabindex=0><code>--cache.ideal-lower-fill-ratio
--cache.ideal-upper-fill-ratio
</code></pre><p>The defaults are <code>0.04</code> and <code>0.25</code> respectively. If you increase these
limits, the hash tables have a higher utilization, which uses
less memory but can potentially harm performance.</p><p>There are a number of options to pre-fill the caches under certain
circumstances:</p><pre tabindex=0><code>--rocksdb.auto-fill-index-caches-on-startup
--rocksdb.auto-refill-index-caches-on-modify
--rocksdb.auto-refill-index-caches-on-followers
--rocksdb.auto-refill-index-caches-queue-capacity
</code></pre><p>The first leads to the caches being automatically filled on startup, the
second on any modifications. Both are set to <code>false</code> by default, so
setting these to <code>true</code> will - in general - use more RAM rather than
less. However, it will not lead to usage of more RAM than the configured
limits for all caches.</p><p>The third option above is by default <code>true</code>, so that caches on
followers will automatically be refilled if any of the first two options
is set to <code>true</code>. Setting this to <code>false</code> can save RAM usage on
followers. Of course, this means that in case of a failover the caches
of the new leader will be cold!</p><p>Finally, the amount of write operations being queued for index refill
can be limited with <code>--rocksdb.auto-refill-index-caches-queue-capacity</code>
to avoid over-allocation if the indexing cannot keep up with the writes.
The default for this value is 131072.</p><h2 id=aql-query-memory-usage>AQL Query Memory Usage <a href=/3.11/operations/administration/reduce-memory-footprint/#aql-query-memory-usage class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>In addition to all the buffers and caches above, AQL queries will use additional
memory during their execution to process your data and build up result sets.
This memory is used during the query execution only and will be released afterwards,
in contrast to the held memory for buffers and caches.</p><p>By default, queries will build up their full results in memory. While you can
fetch the results batch by batch by using a cursor, every query needs to compute
the entire result first before you can retrieve the first batch. The server
also needs to hold the results in memory until the corresponding cursor is fully
consumed or times out. Building up the full results reduces the time the server
has to work with collections at the cost of main memory.</p><p>In ArangoDB version 3.4 we introduced
<a href=/3.11/release-notes/version-3.4/whats-new-in-3-4/#streaming-aql-cursors class=link>streaming cursors</a> with
somewhat inverted properties: less peak memory usage, longer access to the
collections. Streaming is possible on document level, which means that it cannot
be applied to all query parts. For example, a <code>MERGE()</code> of all results of a
subquery cannot be streamed (the result of the operation has to be built up fully).
Nonetheless, the surrounding query may be eligible for streaming.</p><p>Aside from streaming cursors, ArangoDB offers the possibility to specify a
memory limit which a query should not exceed. If it does, the query will be
aborted. Memory statistics are checked between execution blocks, which
correspond to lines in the <em>explain</em> output. That means queries which require
functions may require more memory for intermediate processing, but this will not
kill the query because the memory.
The startup option to restrict the peak memory usage for each AQL query is
<code>--query.memory-limit</code>. This is a per-query limit, i.e. at maximum each AQL query is allowed
to use the configured amount of memory. To set a global memory limit for
all queries together, use the <code>--query.global-memory-limit</code> setting.</p><p>You can also use <em>LIMIT</em> operations in AQL queries to reduce the number of documents
that need to be inspected and processed. This is not always what happens under
the hood, as some operations may lead to an intermediate result being computed before
any limit is applied.</p><h2 id=statistics>Statistics <a href=/3.11/operations/administration/reduce-memory-footprint/#statistics class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The server collects
<a href=/3.11/components/arangodb-server/options/#--serverstatistics class=link>statistics</a> regularly,
which is displayed in the web interface. You will have a light query load every
few seconds, even if your application is idle, because of the statistics. If required, you can
turn it off via:</p><pre tabindex=0><code>--server.statistics false
</code></pre><p>This setting will disable both the background statistics gathering and the statistics
APIs. To only turn off the statistics gathering, you can use</p><pre tabindex=0><code>--server.statistics-history false
</code></pre><p>That leaves all statistics APIs enabled but still disables all background work
done by the statistics gathering.</p><h2 id=javascript--foxx>JavaScript & Foxx <a href=/3.11/operations/administration/reduce-memory-footprint/#javascript--foxx class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p><a href=/3.11/components/arangodb-server/options/#javascript class=link>JavaScript</a> is executed in the ArangoDB
process using the embedded V8 engine:</p><ul><li>Backend parts of the web interface</li><li>Foxx Apps</li><li>Foxx Queues</li><li>GraphQL</li><li>JavaScript-based transactions</li><li>User-defined AQL functions</li></ul><p>There are several <em>V8 contexts</em> for parallel execution. You can think of them as
a thread pool. They are also called <em>isolates</em>. Each isolate has a heap of a few
gigabytes by default. You can restrict V8 if you use no or very little
JavaScript:</p><pre tabindex=0><code>--javascript.v8-contexts 2
--javascript.v8-max-heap 512
</code></pre><p>This will limit the number of V8 isolates to two. All JavaScript related
requests will be queued up until one of the isolates becomes available for the
new task. It also restricts the heap size to 512 MByte, so that both V8 contexts
combined cannot use more than 1 GByte of memory in the worst case.</p><h3 id=v8-for-the-desperate>V8 for the Desperate <a href=/3.11/operations/administration/reduce-memory-footprint/#v8-for-the-desperate class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>You should not use the following settings unless there are very good reasons,
like a local development system on which performance is not critical or an
embedded system with very limited hardware resources!</p><pre tabindex=0><code>--javascript.v8-contexts 1
--javascript.v8-max-heap 256
</code></pre><p>Using the settings above, you can reduce the memory usage of V8 to 256 MB and just
one thread. There is a chance that some operations will be aborted because they run
out of memory in the web interface for instance. Also, JavaScript requests will be
executed one by one.</p><p>If you are very tight on memory, and you are sure that you do not need V8, you
can disable it completely:</p><pre tabindex=0><code>--javascript.enabled false
--foxx.queues false
</code></pre><p>In consequence, the following features will not be available:</p><ul><li>Backend parts of the web interface</li><li>Foxx Apps</li><li>Foxx Queues</li><li>GraphQL</li><li>JavaScript-based transactions</li><li>User-defined AQL functions</li></ul><p>Note that JavaScript / V8 is automatically disabled for DB-Server and Agency
nodes in a cluster without these limitations. They apply only to single server
instances and Coordinator nodes. You should not disable V8 on Coordinators
because certain cluster operations depend on it.</p><h2 id=concurrent-operations>Concurrent operations <a href=/3.11/operations/administration/reduce-memory-footprint/#concurrent-operations class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Starting with ArangoDB 3.8 one can limit the number of concurrent
operations being executed on each Coordinator. Reducing the amount of
concurrent operations can lower the RAM usage on Coordinators. The
startup option for this is:</p><pre tabindex=0><code>--server.ongoing-low-priority-multiplier
</code></pre><p>The default for this option is 4, which means that a Coordinator with <code>t</code>
scheduler threads can execute up to <code>4 * t</code> requests concurrently. The
minimal value for this option is 1.</p><p>Also see the <a href=/3.11/components/arangodb-server/options/#--serverongoing-low-priority-multiplier class=link><em>arangod</em> startup options</a>.</p><h2 id=cpu-usage>CPU usage <a href=/3.11/operations/administration/reduce-memory-footprint/#cpu-usage class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>We cannot really reduce CPU usage, but the number of threads running in parallel.
Again, you should not do this unless there are very good reasons, like an
embedded system. Note that this will limit the performance for concurrent
requests, which may be okay for a local development system with you as only
user.</p><p>The number of background threads can be limited in the following way:</p><pre tabindex=0><code>--arangosearch.threads-limit 1
--rocksdb.max-background-jobs 4
--server.maintenance-threads 3
--server.maximal-threads 5
--server.minimal-threads 1
</code></pre><p>This will usually not be good for performance, though.</p><p>In general, the number of threads is determined automatically to match the
capabilities of the target machine. However, each thread requires at most 8 MB
of stack memory when running ArangoDB on Linux (most of the time a lot less),
so having a lot of concurrent
threads around will need a lot of memory, too.
Reducing the number of server threads as in the example above can help reduce the
memory usage by thread, but will sacrifice throughput.</p><p>In addition, the following option will make logging synchronous, saving one
dedicated background thread for the logging:</p><pre tabindex=0><code>--log.force-direct true
</code></pre><p>This is not recommended unless you only log errors and warnings.</p><h2 id=examples>Examples <a href=/3.11/operations/administration/reduce-memory-footprint/#examples class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>If you don&rsquo;t want to go with the default settings, you should first adjust the
size of the block cache and the edge cache. If you have a graph use case, you
should go for a larger edge cache. For example, split the memory 50:50 between
the block cache and the edge cache. If you have no edges, then go for a minimal
edge cache and use most of the memory for the block cache.</p><p>For example, if you have a machine with 40 GByte of memory and you want to
restrict ArangoDB to 20 GB of that, use 10 GB for the edge cache and 10 GB for
the block cache if you use graph features.</p><p>Please keep in mind that during query execution additional memory will be used
for query results temporarily. If you are tight on memory, you may want to go
for 7 GB each instead.</p><p>If you have an embedded system or your development laptop, you can use all of
the above settings to lower the memory footprint further. For normal operation,
especially production, these settings are not recommended.</p><h2 id=linux-system-configuration>Linux System Configuration <a href=/3.11/operations/administration/reduce-memory-footprint/#linux-system-configuration class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The main deployment target for ArangoDB is Linux. As you have learned above
ArangoDB and its innards work a lot with memory. Thus its vital to know how
ArangoDB and the Linux kernel interact on that matter. The linux kernel offers
several modes of how it will manage memory. You can influence this via the proc
filesystem, the file <code>/etc/sysctl.conf</code> or a file in <code>/etc/sysctl.conf.d/</code> which
your system will apply to the kernel settings at boot time. The settings as
named below are intended for the sysctl infrastructure, meaning that they map
to the <code>proc</code> filesystem as <code>/proc/sys/vm/overcommit_memory</code>.</p><p>A <code>vm.overcommit_memory</code> setting of <strong>2</strong> can cause issues in some environments
in combination with the bundled memory allocator ArangoDB ships with (jemalloc).</p><p>The allocator demands consecutive blocks of memory from the kernel, which are
also mapped to on-disk blocks. This is done on behalf of the server process
(<em>arangod</em>). The process may use some chunks of a block for a long time span, but
others only for a short while and therefore release the memory. It is then up to
the allocator to return the freed parts back to the kernel. Because it can only
give back consecutive blocks of memory, it has to split the large block into
multiple small blocks and can then return the unused ones.</p><p>With an <code>vm.overcommit_memory</code> kernel settings value of <strong>2</strong>, the allocator may
have trouble with splitting existing memory mappings, which makes the <em>number</em>
of memory mappings of an arangod server process grow over time. This can lead to
the kernel refusing to hand out more memory to the arangod process, even if more
physical memory is available. The kernel will only grant up to <code>vm.max_map_count</code>
memory mappings to each process, which defaults to 65530 on many Linux
environments.</p><p>Another issue when running jemalloc with <code>vm.overcommit_memory</code> set to <strong>2</strong> is
that for some workloads the amount of memory that the Linux kernel tracks as
<em>committed memory</em> also grows over time and never decreases. Eventually,
<em>arangod</em> may not get any more memory simply because it reaches the configured
overcommit limit (physical RAM * <code>overcommit_ratio</code> + swap space).</p><p>The solution is to
<a href=/3.11/operations/installation/linux/operating-system-configuration/#overcommit-memory class=link>modify the value of <code>vm.overcommit_memory</code></a>
from <strong>2</strong> to either <strong>0</strong> or <strong>1</strong>. This will fix both of these problems.
We still observe ever-increasing <em>virtual memory</em> consumption when using
jemalloc regardless of the overcommit setting, but in practice this should not
cause any issues. <strong>0</strong> is the Linux kernel default and also the setting we recommend.</p><p>For the sake of completeness, let us also mention another way to address the
problem: use a different memory allocator. This requires to compile ArangoDB
from the source code without jemalloc (<code>-DUSE_JEMALLOC=Off</code> in the call to cmake).
With the system&rsquo;s libc allocator you should see quite stable memory usage. We
also tried another allocator, precisely the one from <code>libmusl</code>, and this also
shows quite stable memory usage over time. What holds us back to change the
bundled allocator are that it is a non-trivial change and because jemalloc has
very nice performance characteristics for massively multi-threaded processes
otherwise.</p><h2 id=testing-the-effects-of-reduced-io-buffers>Testing the Effects of Reduced I/O Buffers <a href=/3.11/operations/administration/reduce-memory-footprint/#testing-the-effects-of-reduced-io-buffers class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p><figure class=image-caption><img alt="Performance Graph" src=/images/performance_graph.png><figcaption></figcaption></figure></p><ul><li>15:50 – Start bigger import</li><li>16:00 – Start writing documents of ~60 KB size one at a time</li><li>16:45 – Add similar second writer</li><li>16:55 – Restart ArangoDB with the RocksDB write buffer configuration suggested above</li><li>17:20 – Buffers are full, write performance drops</li><li>17:38 – WAL rotation</li></ul><p>What you see in above performance graph are the consequences of restricting the
write buffers. Until we reach a 90% fill rate of the write buffers the server
can almost follow the load pattern for a while at the cost of constantly
increasing buffers. Once RocksDB reaches 90% buffer fill rate, it will
significantly throttle the load to ~50%. This is expected according to the
<a href=https://github.com/facebook/rocksdb/wiki/Write-Buffer-Manager target=_blank rel="noopener noreferrer" class=link>upstream documentation</a>&nbsp;<i class="fas fa-external-link-alt"></i>:</p><blockquote><p>[…] a flush will be triggered […] if total mutable memtable size exceeds 90%
of the limit. If the actual memory is over the limit, more aggressive flush
may also be triggered even if total mutable memtable size is below 90%.</p></blockquote><p>Since we only measured the disk I/O bytes, we do not see that the document save
operations also doubled in request time.</p><nav class=pagination><span class=prev><a class="nav nav-prev link" href=/3.11/operations/administration/arangodb-starter/><i class="fas fa-chevron-left fa-fw"></i><p>ArangoDB Starter Administration</p></a></span><span class=next><a class="nav nav-next link" href=/3.11/operations/security/><p>Security</p><i class="fas fa-chevron-right fa-fw"></i></a></span></nav></article><div class=toc-container><a class=edit-page aria-label href=https://github.com/arangodb/docs-hugo/edit/main/site/content/3.11/operations/administration/reduce-memory-footprint.md target=_blank><i class="fab fa-fw fa-github edit-page-icon"></i></a><div class=toc><div class=toc-content><div class=toc-header><p>On this page</p></div><nav id=TableOfContents><div class=level-2><a href=#limiting-the-overall-ram-usage>Limiting the overall RAM usage</a></div><div class=level-2><a href=#overview-over-ram-usage-in-arangodb>Overview over RAM usage in ArangoDB</a></div><div class=level-2><a href=#write-ahead-log-wal>Write ahead log (WAL)</a></div><div class=level-2><a href=#write-buffers>Write Buffers</a></div><div class=level-2><a href=#rocksdb-block-cache>RocksDB Block Cache</a></div><div class=level-2><a href=#transactions>Transactions</a></div><div class=level-2><a href=#rocksdb-compactions>RocksDB compactions</a></div><div class=level-2><a href=#scheduler-queues>Scheduler queues</a></div><div class=level-2><a href=#index-caches>Index Caches</a></div><div class=level-2><a href=#aql-query-memory-usage>AQL Query Memory Usage</a></div><div class=level-2><a href=#statistics>Statistics</a></div><div class=level-2><a href=#javascript--foxx>JavaScript & Foxx</a></div><div class=level-3><a href=#v8-for-the-desperate>V8 for the Desperate</a></div><div class=level-2><a href=#concurrent-operations>Concurrent operations</a></div><div class=level-2><a href=#cpu-usage>CPU usage</a></div><div class=level-2><a href=#examples>Examples</a></div><div class=level-2><a href=#linux-system-configuration>Linux System Configuration</a></div><div class=level-2><a href=#testing-the-effects-of-reduced-io-buffers>Testing the Effects of Reduced I/O Buffers</a></div></nav></div></div></div></div></div></section></section></div><button class="back-to-top hidden" onclick=goToTop(event) href=#><i class="fa fa-arrow-up"></i></button><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@docsearch/css@3>
<script src=https://cdn.jsdelivr.net/npm/@docsearch/js@3></script>
<script type=text/javascript>window.setupDocSearch=function(e){if(!window.docsearch)return;docsearch({appId:"OK3ZBQ5982",apiKey:"500c85ccecb335d507fe4449aed12e1d",indexName:"arangodbdocs",insights:!0,container:"#searchbox",debug:!1,maxResultsPerGroup:10,searchParameters:{facetFilters:[`version:${e}`]}})}</script></body></html>