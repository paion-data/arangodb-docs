<!doctype html><html lang=en><head><link href=//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.no-icons.min.css rel=stylesheet><link href=//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css rel=stylesheet><link href=/css/fontawesome-all.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css rel=stylesheet></noscript><link href=/css/featherlight.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/featherlight.min.css rel=stylesheet></noscript><link href=/css/nucleus.css rel=stylesheet><link href=/css/fonts.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fonts.css rel=stylesheet></noscript><link href=/css/theme.css rel=stylesheet><link href=/css/theme-relearn-light.css rel=stylesheet id=variant-style><link href=/css/print.css rel=stylesheet media=print><script src=/js/variant.js?1743774193></script>
<script>var root_url="/",baseUriFull,baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="",window.T_Copied_to_clipboard="",window.T_Copy_link_to_clipboard="",window.T_Link_copied_to_clipboard="",baseUriFull="http://localhost/",window.variants&&variants.init(["relearn-light"])</script><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.119.0"><meta itemprop=description property="description" content="ArangoDB Datasource for Apache Spark allows batch reading and writing Spark DataFrame data"><meta property="og:url" content="http://localhost/3.11/develop/integrations/arangodb-datasource-for-apache-spark/"><meta property="og:title" content="ArangoDB Datasource for Apache Spark"><meta property="og:type" content="website"><meta property="og:description" content="ArangoDB Datasource for Apache Spark allows batch reading and writing Spark DataFrame data"><meta name=docsearch:version content="3.11"><title>ArangoDB Datasource for Apache Spark | ArangoDB Documentation</title><link href=/images/favicon.png rel=icon type=image/png><script src=/js/jquery.min.js></script>
<script src=/js/clipboard.min.js?1743774193 defer></script>
<script src=/js/featherlight.min.js?1743774193 defer></script>
<script>var versions=[{alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"},{alias:"stable",deprecated:!1,name:"3.12",version:"3.12.4"},{alias:"3.11",deprecated:!1,name:"3.11",version:"3.11.13"},{alias:"3.10",deprecated:!0,name:"3.10",version:"3.10.14"}]</script><script>var develVersion={alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"}</script><script>var stableVersion={alias:"stable",deprecated:!1,name:"3.12",version:"3.12.4"}</script><script src=/js/codeblocks.js?1743774193 defer></script>
<script src=/js/theme.js?1743774193 defer></script></head><body><noscript>You need to enable JavaScript to use the ArangoDB documentation.</noscript><div id=page-wrapper class=page_content_splash style=height:auto;opacity:0><section id=page-main><section class=page-container id=page-container><header id=header style="transition:.5s padding ease-out,.15s" class="zn_header_white header-splash-new nav-down header-splash-wrap header1"><div class=header-block-left><div class=mobile-menu-toggle><button id=sidebar-toggle-navigation onclick=showSidebarHandler()><svg width="1.33em" height="1.33em" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button></div><div class=version-logo-container><div class="logo-container hasinfocard_img arangodb-logo-large"><div class=logo><a href=https://www.arangodb.com/><img src=/images/logo_main.png alt=ArangoDB title></a></div></div><div class=arangodb-logo-small><a href=https://arangodb.com/><img alt="ArangoDB Logo" src=/images/ArangoDB_Logo_White_small.png></a></div></div></div><div class=container-right style=display:hidden></div><div class=search-and-version-container><a href=# class="home-link is-current" aria-label="Go to home page" onclick=goToHomepage(event)></a><div id=searchbox></div><script type=text/javascript>const SCRIPT_SRC="https://unpkg.com/@inkeep/widgets-embed@0.2.290/dist/embed.js";function loadAndInitializeInkeep(){if(document.querySelector(`script[src="${SCRIPT_SRC}]"`))return;const e=document.createElement("script");e.type="module",e.src=SCRIPT_SRC,e.onload=initializeInkeep,document.head.appendChild(e)}function initializeInkeep(){const e=Inkeep({integrationId:"clo4lx6jk0000s601cp21x2ok",apiKey:"13b4e56966a76e86c6ff359cd795ee6a0412f751d75d6383",organizationId:"org_HGBkkzGAa4KeGJGh",organizationDisplayName:"ArangoDB",primaryBrandColor:"#80a54d",stringReplacementRules:[{matchingRule:{ruleType:"Substring",string:"Arangograph"},replaceWith:"ArangoGraph"},{matchingRule:{ruleType:"Substring",string:"Aql"},replaceWith:"AQL"},{matchingRule:{ruleType:"Substring",string:"Arangodb"},replaceWith:"ArangoDB"}],customCardSettings:[{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arango.qubitpi.org"}},searchTabLabel:"Official Docs"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"developer.arangodb.com"}},searchTabLabel:"Developer Hub"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arangodb.com"}},searchTabLabel:"Home"}]}),t=e.embed({componentType:"ChatButton",properties:{stylesheetUrls:["/css/fonts.css"],fixedPositionXOffset:"52px",baseSettings:{theme:{primaryColors:{textColorOnPrimary:"white"},tokens:{fonts:{body:"'Inter'",heading:"'Inter'"},zIndex:{overlay:1e4,modal:11e3,popover:12e3,skipLink:13e3,toast:14e3,tooltip:15e3}}}},aiChatSettings:{botAvatarSrcUrl:"/images/ArangoDB_Logo_White_small.png",quickQuestions:["What can you do with AQL that is not feasible with SQL?","How do I search for objects within arrays?","Where can I deploy my ArangoDB instance?"],getHelpCallToActions:[{icon:{builtIn:"FaSlack"},name:"Slack",url:"https://arangodb-community.slack.com/"}]},searchSettings:{tabSettings:{isAllTabEnabled:!1,alwaysDisplayedTabs:["Official Docs","Developer Hub","Home"]}}}})}loadAndInitializeInkeep()</script><div class=version-selector><select id=arangodb-version onchange=changeVersion()><option value=3.13>3.13</option><option value=3.12>3.12</option><option value=3.11>3.11</option><option value=3.10>3.10</option><option value=3.9>3.9</option><option value=3.8>3.8</option></select></div></div></header><iframe src=/nav.html title=description id=menu-iframe class="menu-iframe active" style=opacity:0></iframe><div class=container-main><div class=row-main><nav id=breadcrumbs><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><meta itemprop=itemListOrder content="Descending"><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="3.11"><a itemprop=item class=link href=/3.11/><span itemprop=name class=breadcrumb-entry>3.11.13</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Develop"><a itemprop=item class=link href=/3.11/develop/><span itemprop=name class=breadcrumb-entry>Develop</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Integrations"><a itemprop=item class=link href=/3.11/develop/integrations/><span itemprop=name class=breadcrumb-entry>Integrations</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Datasource for Apache Spark"><a itemprop=item class=link href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/><span itemprop=name class=breadcrumb-entry>Datasource for Apache Spark</span></a></li></ol></nav><article class=default><hgroup><h1>ArangoDB Datasource for Apache Spark</h1><p class=lead>ArangoDB Datasource for Apache Spark allows batch reading and writing Spark DataFrame data</p></hgroup><p>ArangoDB Datasource for Apache Spark allows batch reading and writing Spark DataFrame data from and to ArangoDB, by implementing the Spark Data Source V2 API.</p><p>Reading tasks are parallelized according to the number of shards of the related ArangoDB collection, and the writing ones - depending on the source DataFrame partitions. The network traffic is load balanced across the available DB Coordinators.</p><p>Filter predicates and column selections are pushed down to the DB by dynamically generating AQL queries, which will fetch only the strictly required data, thus saving network and computational resources both on the Spark and the DB side.</p><p>The connector is usable from all the Spark supported client languages: Scala, Python, Java, and R.</p><p>This library works with all the non-EOLed <a href=https://www.arangodb.com/subscriptions/end-of-life-notice/ target=_blank rel="noopener noreferrer" class=link>ArangoDB versions</a>&nbsp;<i class="fas fa-external-link-alt"></i>.</p><h2 id=supported-versions>Supported versions <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#supported-versions class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>There are several variants of this library, each one compatible with different Spark and Scala versions:</p><ul><li><code>com.arangodb:arangodb-spark-datasource-3.3_2.12</code> (Spark 3.3, Scala 2.12)</li><li><code>com.arangodb:arangodb-spark-datasource-3.3_2.13</code> (Spark 3.3, Scala 2.13)</li><li><code>com.arangodb:arangodb-spark-datasource-3.4_2.12</code> (Spark 3.4, Scala 2.12) (compatible with Spark <code>3.4.2+</code>)</li><li><code>com.arangodb:arangodb-spark-datasource-3.4_2.13</code> (Spark 3.4, Scala 2.13) (compatible with Spark <code>3.4.2+</code>)</li><li><code>com.arangodb:arangodb-spark-datasource-3.5_2.12</code> (Spark 3.5, Scala 2.12)</li><li><code>com.arangodb:arangodb-spark-datasource-3.5_2.13</code> (Spark 3.5, Scala 2.13)</li></ul><p>The following variants are no longer supported:</p><ul><li><code>com.arangodb:arangodb-spark-datasource-2.4_2.11</code> (Spark 2.4, Scala 2.11)</li><li><code>com.arangodb:arangodb-spark-datasource-2.4_2.12</code> (Spark 2.4, Scala 2.12)</li><li><code>com.arangodb:arangodb-spark-datasource-3.1_2.12</code> (Spark 3.1, Scala 2.12)</li><li><code>com.arangodb:arangodb-spark-datasource-3.2_2.12</code> (Spark 3.2, Scala 2.12)</li><li><code>com.arangodb:arangodb-spark-datasource-3.2_2.13</code> (Spark 3.2, Scala 2.13)</li></ul><p>Since version <code>1.7.0</code>, due to <a href=https://github.com/apache/spark/commit/ad29290a02fb94a958fd21e301100338c9f5b82a#diff-b25c8acff88c1b4850c6642e80845aac4fb882c664795c3b0aa058e37ed732a0L42-R52 target=_blank rel="noopener noreferrer" class=link>breaking changes</a>&nbsp;<i class="fas fa-external-link-alt"></i>
in Spark <code>3.4.2</code>, <code>arangodb-spark-datasource-3.4</code> is not compatible anymore with Spark versions <code>3.4.0</code> and <code>3.4.1</code>.</p><p>In the following sections the <code>${sparkVersion}</code> and <code>${scalaVersion}</code> placeholders refer to the Spark and Scala versions.</p><h2 id=setup>Setup <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#setup class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>To import ArangoDB Datasource for Apache Spark in a Maven project:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl>  <span class=nt>&lt;dependencies&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;dependency&gt;</span>
</span></span><span class=line><span class=cl>      <span class=nt>&lt;groupId&gt;</span>com.arangodb<span class=nt>&lt;/groupId&gt;</span>
</span></span><span class=line><span class=cl>      <span class=nt>&lt;artifactId&gt;</span>arangodb-spark-datasource-${sparkVersion}_${scalaVersion}<span class=nt>&lt;/artifactId&gt;</span>
</span></span><span class=line><span class=cl>      <span class=nt>&lt;version&gt;</span>x.y.z<span class=nt>&lt;/version&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;/dependency&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/dependencies&gt;</span>
</span></span></code></pre></div><p>To use in an external Spark cluster, submit your application with the following parameter:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>--packages<span class=o>=</span><span class=s2>&#34;com.arangodb:arangodb-spark-datasource-</span><span class=si>${</span><span class=nv>sparkVersion</span><span class=si>}</span><span class=s2>_</span><span class=si>${</span><span class=nv>scalaVersion</span><span class=si>}</span><span class=s2>:x.y.z&#34;</span>
</span></span></code></pre></div><h2 id=general-configuration>General Configuration <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#general-configuration class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><ul><li><code>user</code>: db user, <code>root</code> by default</li><li><code>password</code>: db password</li><li><code>endpoints</code>: list of Coordinators, e.g. <code>c1:8529,c2:8529</code> (required)</li><li><code>acquireHostList</code>: acquire the list of all known hosts in the cluster (<code>true</code> or <code>false</code>), <code>false</code> by default</li><li><code>protocol</code>: communication protocol (<code>vst</code>, <code>http</code>, or <code>http2</code>), <code>http2</code> by default</li><li><code>contentType</code>: content type for driver communication (<code>json</code> or <code>vpack</code>), <code>json</code> by default</li><li><code>timeout</code>: driver connect and request timeout in ms, <code>300000</code> by default</li><li><code>ssl.enabled</code>: ssl secured driver connection (<code>true</code> or <code>false</code>), <code>false</code> by default</li><li><code>ssl.verifyHost</code>: whether TLS hostname verification is enabled, <code>true</code> by default</li><li><code>ssl.cert.value</code>: Base64 encoded certificate</li><li><code>ssl.cert.type</code>: certificate type, <code>X.509</code> by default</li><li><code>ssl.cert.alias</code>: certificate alias name, <code>arangodb</code> by default</li><li><code>ssl.algorithm</code>: trust manager algorithm, <code>SunX509</code> by default</li><li><code>ssl.keystore.type</code>: keystore type, <code>jks</code> by default</li><li><code>ssl.protocol</code>: SSLContext protocol, <code>TLS</code> by default</li><li><code>ssl.trustStore.path</code>: trust store path</li><li><code>ssl.trustStore.password</code>: trust store password</li></ul><h3 id=ssl>SSL <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#ssl class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>To use TLS-secured connections to ArangoDB, set <code>ssl.enabled</code> to <code>true</code> and
configure the certificate to use. This can be achieved in one of the following ways:</p><ul><li><p>Provide the Base64-encoded certificate as the <code>ssl.cert.value</code> configuration entry:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>spark</span><span class=k>:</span> <span class=kt>SparkSession</span> <span class=o>=</span> <span class=nc>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;ssl.enabled&#34;</span><span class=o>,</span> <span class=s>&#34;true&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;ssl.cert.value&#34;</span><span class=o>,</span> <span class=s>&#34;&lt;Base64-encoded certificate&gt;&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>getOrCreate</span><span class=o>()</span>
</span></span></code></pre></div></li><li><p>Set the trust store to use in the <code>ssl.trustStore.path</code> configuration entry and
optionally set <code>ssl.trustStore.password</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>spark</span><span class=k>:</span> <span class=kt>SparkSession</span> <span class=o>=</span> <span class=nc>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;ssl.enabled&#34;</span><span class=o>,</span> <span class=s>&#34;true&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;ssl.trustStore.path&#34;</span><span class=o>,</span> <span class=s>&#34;&lt;trustStore path&gt;&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;ssl.trustStore.password&#34;</span><span class=o>,</span> <span class=s>&#34;&lt;trustStore password&gt;&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>getOrCreate</span><span class=o>()</span>
</span></span></code></pre></div></li><li><p>Start the Spark driver and workers with a properly configured trust store:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>spark</span><span class=k>:</span> <span class=kt>SparkSession</span> <span class=o>=</span> <span class=nc>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;ssl.enabled&#34;</span><span class=o>,</span> <span class=s>&#34;true&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>getOrCreate</span><span class=o>()</span>
</span></span></code></pre></div><p>Set the following in the Spark configuration file:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-properties data-lang=properties><span class=line><span class=cl><span class=na>spark.executor.extraJavaOptions</span><span class=o>=</span><span class=s>-Djavax.net.ssl.trustStore=&lt;trustStore path&gt; -Djavax.net.ssl.trustStorePassword=&lt;trustStore password&gt; </span>
</span></span><span class=line><span class=cl><span class=na>spark.driver.extraJavaOptions</span><span class=o>=</span><span class=s>-Djavax.net.ssl.trustStore=&lt;trustStore path&gt; -Djavax.net.ssl.trustStorePassword=&lt;trustStore password&gt;</span>
</span></span></code></pre></div><p>Alternatively, you can set this in the command-line when submitting the Spark job:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>./bin/spark-submit <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --conf <span class=s2>&#34;spark.driver.extraJavaOptions=-Djavax.net.ssl.trustStore=&lt;trustStore path&gt; -Djavax.net.ssl.trustStorePassword=&lt;trustStore password&gt;&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --conf <span class=s2>&#34;spark.executor.extraJavaOptions=-Djavax.net.ssl.trustStore=&lt;trustStore path&gt; -Djavax.net.ssl.trustStorePassword=&lt;trustStore password&gt;&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  ...
</span></span></code></pre></div></li></ul><h3 id=supported-deployment-topologies>Supported deployment topologies <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#supported-deployment-topologies class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>The connector can work with a single server, a cluster and active failover deployments of ArangoDB.</p><h2 id=batch-read>Batch Read <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#batch-read class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The connector implements support for batch reading from an ArangoDB collection.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span><span class=k>:</span> <span class=kt>DataFrame</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>format</span><span class=o>(</span><span class=s>&#34;com.arangodb.spark&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>options</span><span class=o>(</span><span class=n>options</span><span class=o>)</span> <span class=c1>// Map[String, String]
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=o>.</span><span class=n>schema</span><span class=o>(</span><span class=n>schema</span><span class=o>)</span> <span class=c1>// StructType
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=o>.</span><span class=n>load</span><span class=o>()</span>
</span></span></code></pre></div><p>The connector can read data from:</p><ul><li>a collection</li><li>an AQL cursor (query specified by the user)</li></ul><p>When reading data from a <strong>collection</strong>, the reading job is split into many Spark tasks, one for each shard in the ArangoDB source collection. The resulting Spark DataFrame has the same number of partitions as the number of shards in the ArangoDB collection, each one containing data from the respective collection shard. The reading tasks consist of AQL queries that are load balanced across all the available ArangoDB Coordinators. Each query is related to only one shard, therefore it will be executed locally in the DB-Server holding the related shard.</p><p>When reading data from an <strong>AQL cursor</strong>, the reading job cannot be partitioned or parallelized, so it will be less scalable. This mode can be used for reading data coming from different tables, i.e. resulting from an AQL traversal query.</p><p><strong>Example</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>spark</span><span class=k>:</span> <span class=kt>SparkSession</span> <span class=o>=</span> <span class=nc>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>appName</span><span class=o>(</span><span class=s>&#34;ArangoDBSparkDemo&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>master</span><span class=o>(</span><span class=s>&#34;local[*]&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&#34;spark.driver.host&#34;</span><span class=o>,</span> <span class=s>&#34;127.0.0.1&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>getOrCreate</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span><span class=k>:</span> <span class=kt>DataFrame</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>format</span><span class=o>(</span><span class=s>&#34;com.arangodb.spark&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>options</span><span class=o>(</span><span class=nc>Map</span><span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=s>&#34;password&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;test&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>    <span class=s>&#34;endpoints&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;c1:8529,c2:8529,c3:8529&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>    <span class=s>&#34;table&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;users&#34;</span>
</span></span><span class=line><span class=cl>  <span class=o>))</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>schema</span><span class=o>(</span><span class=k>new</span> <span class=nc>StructType</span><span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=nc>Array</span><span class=o>(</span>
</span></span><span class=line><span class=cl>      <span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;likes&#34;</span><span class=o>,</span> <span class=nc>ArrayType</span><span class=o>(</span><span class=nc>StringType</span><span class=o>,</span> <span class=n>containsNull</span> <span class=k>=</span> <span class=kc>false</span><span class=o>)),</span>
</span></span><span class=line><span class=cl>      <span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;birthday&#34;</span><span class=o>,</span> <span class=nc>DateType</span><span class=o>,</span> <span class=n>nullable</span> <span class=k>=</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl>      <span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;gender&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=n>nullable</span> <span class=k>=</span> <span class=kc>false</span><span class=o>),</span>
</span></span><span class=line><span class=cl>      <span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;name&#34;</span><span class=o>,</span> <span class=nc>StructType</span><span class=o>(</span>
</span></span><span class=line><span class=cl>        <span class=nc>Array</span><span class=o>(</span>
</span></span><span class=line><span class=cl>          <span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;first&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=n>nullable</span> <span class=k>=</span> <span class=kc>true</span><span class=o>),</span>
</span></span><span class=line><span class=cl>          <span class=nc>StructField</span><span class=o>(</span><span class=s>&#34;last&#34;</span><span class=o>,</span> <span class=nc>StringType</span><span class=o>,</span> <span class=n>nullable</span> <span class=k>=</span> <span class=kc>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>)</span>
</span></span><span class=line><span class=cl>      <span class=o>),</span> <span class=n>nullable</span> <span class=k>=</span> <span class=kc>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>))</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>load</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>usersDF</span><span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>col</span><span class=o>(</span><span class=s>&#34;birthday&#34;</span><span class=o>)</span> <span class=o>===</span> <span class=s>&#34;1982-12-15&#34;</span><span class=o>).</span><span class=n>show</span><span class=o>()</span>
</span></span></code></pre></div><h3 id=read-configuration>Read Configuration <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#read-configuration class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><ul><li><code>database</code>: database name, <code>_system</code> by default</li><li><code>table</code>: datasource ArangoDB collection name, ignored if <code>query</code> is specified. Either <code>table</code> or <code>query</code> is required.</li><li><code>query</code>: custom AQL read query. If set, <code>table</code> will be ignored. Either <code>table</code> or <code>query</code> is required.</li><li><code>batchSize</code>: reading batch size, <code>10000</code> by default</li><li><code>sampleSize</code>: sample size prefetched for schema inference, only used if read schema is not provided, <code>1000</code> by default</li><li><code>fillBlockCache</code>: specifies whether the query should store the data it reads in the RocksDB block cache (<code>true</code> or <code>false</code>), <code>false</code> by default</li><li><code>stream</code>: specifies whether the query should be executed lazily, <code>true</code> by default</li><li><code>ttl</code>: cursor time to live in seconds, <code>30</code> by default</li><li><code>mode</code>: allows setting a mode for dealing with corrupt records during parsing:<ul><li><code>PERMISSIVE</code> : win case of a corrupted record, the malformed string is put into a field configured by
<code>columnNameOfCorruptRecord</code>, and sets malformed fields to null. To keep corrupt records, a user can set a string
type field named <code>columnNameOfCorruptRecord</code> in a user-defined schema. If a schema does not have the field, it drops
corrupt records during parsing. When inferring a schema, it implicitly adds the <code>columnNameOfCorruptRecord</code> field in
an output schema</li><li><code>DROPMALFORMED</code>: ignores the whole corrupted records</li><li><code>FAILFAST</code>: throws an exception in case of corrupted records</li></ul></li><li><code>columnNameOfCorruptRecord</code>: allows renaming the new field having malformed string created by the <code>PERMISSIVE</code> mode</li></ul><h3 id=predicate-and-projection-pushdown>Predicate and Projection Pushdown <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#predicate-and-projection-pushdown class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>The connector can convert some Spark SQL filter predicates into AQL predicates and push their execution down to the data source. In this way, ArangoDB can apply the filters and return only the matching documents.</p><p>The following filter predicates (implementations of <code>org.apache.spark.sql.sources.Filter</code>) are pushed down:</p><ul><li><code>And</code></li><li><code>Or</code></li><li><code>Not</code></li><li><code>EqualTo</code></li><li><code>EqualNullSafe</code></li><li><code>IsNull</code></li><li><code>IsNotNull</code></li><li><code>GreaterThan</code></li><li><code>GreaterThanOrEqualFilter</code></li><li><code>LessThan</code></li><li><code>LessThanOrEqualFilter</code></li><li><code>StringStartsWithFilter</code></li><li><code>StringEndsWithFilter</code></li><li><code>StringContainsFilter</code></li><li><code>InFilter</code></li></ul><p>Furthermore, the connector will push down the subset of columns required by the Spark query, so that only the relevant documents fields will be returned.</p><p>Predicate and projection pushdowns are only performed while reading an ArangoDB collection (set by the <code>table</code> configuration parameter). In case of a batch read from a custom query (set by the <code>query</code> configuration parameter), no pushdown optimizations are performed.</p><h3 id=read-resiliency>Read Resiliency <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#read-resiliency class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>The data of each partition is read using an AQL cursor. If any error occurs, the read task of the related partition will fail. Depending on the Spark configuration, the task could be retried.</p><h2 id=batch-write>Batch Write <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#batch-write class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The connector implements support for batch writing to ArangoDB collection.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>import</span> <span class=nn>org.apache.spark.sql.DataFrame</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span><span class=k>:</span> <span class=kt>DataFrame</span> <span class=o>=</span> <span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>df</span><span class=o>.</span><span class=n>write</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>format</span><span class=o>(</span><span class=s>&#34;com.arangodb.spark&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>mode</span><span class=o>(</span><span class=nc>SaveMode</span><span class=o>.</span><span class=nc>Append</span><span class=o>)</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>options</span><span class=o>(</span><span class=nc>Map</span><span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=s>&#34;password&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;test&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>    <span class=s>&#34;endpoints&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;c1:8529,c2:8529,c3:8529&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>    <span class=s>&#34;table&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;users&#34;</span>
</span></span><span class=line><span class=cl>  <span class=o>))</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>save</span><span class=o>()</span>
</span></span></code></pre></div><p>Write tasks are load balanced across the available ArangoDB Coordinators. The data saved into the ArangoDB is sharded according to the related target collection definition and is different from the Spark DataFrame partitioning.</p><h3 id=savemode>SaveMode <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#savemode class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>On writing, <code>org.apache.spark.sql.SaveMode</code> is used to specify the expected behavior in case the target collection already exists.</p><p>The following save modes are supported:</p><ul><li><code>Append</code>: the target collection is created, if it does not exist.</li><li><code>Overwrite</code>: the target collection is created, if it does not exist, otherwise it is truncated. Use it in combination with the
<code>confirmTruncate</code> write configuration parameter.</li></ul><p>Save modes <code>ErrorIfExists</code> and <code>Ignore</code> behave the same as <code>Append</code>.</p><p>Use the <code>overwriteMode</code> write configuration parameter to specify the document overwrite behavior (if a document with the same <code>_key</code> already exists).</p><h3 id=write-configuration>Write Configuration <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#write-configuration class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><ul><li><code>database</code>: database name, <code>_system</code> by default</li><li><code>table</code>: target ArangoDB collection name (required)</li><li><code>batchSize</code>: writing batch size, <code>10000</code> by default</li><li><code>byteBatchSize</code>: byte batch size threshold, only considered for <code>contentType=json</code>, <code>8388608</code> by default (8 MB)</li><li><code>table.shards</code>: number of shards of the created collection (in case of the <code>Append</code> or <code>Overwrite</code> SaveMode)</li><li><code>table.type</code>: type (<code>document</code> or <code>edge</code>) of the created collection (in case of the <code>Append</code> or <code>Overwrite</code> SaveMode), <code>document</code> by default</li><li><code>waitForSync</code>: specifies whether to wait until the documents have been synced to disk (<code>true</code> or <code>false</code>), <code>false</code> by default</li><li><code>confirmTruncate</code>: confirms to truncate table when using the <code>Overwrite</code> SaveMode, <code>false</code> by default</li><li><code>overwriteMode</code>: configures the behavior in case a document with the specified <code>_key</code> value already exists. It is only considered for <code>Append</code> SaveMode.<ul><li><code>ignore</code> (default for SaveMode other than <code>Append</code>): it will not be written</li><li><code>replace</code>: it will be overwritten with the specified document value</li><li><code>update</code>: it will be patched (partially updated) with the specified document value. The overwrite mode can be
further controlled via the <code>keepNull</code> and <code>mergeObjects</code> parameter. <code>keepNull</code> will also be automatically set to
<code>true</code>, so that null values are kept in the saved documents and not used to remove existing document fields (as for
default ArangoDB upsert behavior).</li><li><code>conflict</code> (default for the <code>Append</code> SaveMode): return a unique constraint violation error so that the insert operation fails</li></ul></li><li><code>mergeObjects</code>: in case <code>overwriteMode</code> is set to <code>update</code>, controls whether objects (not arrays) will be merged.<ul><li><code>true</code> (default): objects will be merged</li><li><code>false</code>: existing document fields will be overwritten</li></ul></li><li><code>keepNull</code>: in case <code>overwriteMode</code> is set to <code>update</code><ul><li><code>true</code> (default): <code>null</code> values are saved within the document (by default)</li><li><code>false</code>: <code>null</code> values are used to delete the corresponding existing attributes</li></ul></li><li><code>retry.maxAttempts</code>: max attempts for retrying write requests in case they are idempotent, <code>10</code> by default</li><li><code>retry.minDelay</code>: min delay in ms between write requests retries, <code>0</code> by default</li><li><code>retry.maxDelay</code>: max delay in ms between write requests retries, <code>0</code> by default</li></ul><h3 id=write-resiliency>Write Resiliency <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#write-resiliency class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>The data of each partition is saved in batches using the ArangoDB API for
<a href=/3.11/develop/http-api/documents/#multiple-document-operations class=link>inserting multiple documents</a>.
This operation is not atomic, therefore some documents could be successfully written to the database, while others could fail. To make the job more resilient to temporary errors (i.e. connectivity problems), in case of failure the request will be retried (with another Coordinator), if the provided configuration allows idempotent requests, namely:</p><ul><li>the schema of the dataframe has a <strong>not nullable</strong> <code>_key</code> field and</li><li><code>overwriteMode</code> is set to one of the following values:<ul><li><code>replace</code></li><li><code>ignore</code></li><li><code>update</code> with <code>keep.null=true</code></li></ul></li></ul><p>A failing batch-saving request is retried once for every Coordinator. After that, if still failing, the write task for the related partition is aborted. According to the Spark configuration, the task can be retried and rescheduled on a different executor, if the provided write configuration allows idempotent requests (as described above).</p><p>If a task ultimately fails and is aborted, the entire write job will be aborted as well. Depending on the <code>SaveMode</code> configuration, the following cleanup operations will be performed:</p><ul><li><code>Append</code>: no cleanup is performed and the underlying data source may require manual cleanup.
<code>DataWriteAbortException</code> is thrown.</li><li><code>Overwrite</code>: the target collection will be truncated.</li><li><code>ErrorIfExists</code>: the target collection will be dropped.</li><li><code>Ignore</code>: if the collection did not exist before, it will be dropped; otherwise, nothing will be done.</li></ul><h3 id=write-requirements>Write requirements <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#write-requirements class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>When writing to an edge collection (<code>table.type=edge</code>), the schema of the Dataframe being written must have:</p><ul><li>a non nullable string field named <code>_from</code>, and</li><li>a non nullable string field named <code>_to</code></li></ul><h3 id=write-limitations>Write Limitations <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#write-limitations class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><ul><li>Batch writes are not performed atomically, so sometimes (i.e. in case of <code>overwrite.mode: conflict</code>) several documents in the batch may be written and others may return an exception (i.e. due to a conflicting key).</li><li>Writing records with the <code>_key</code> attribute is only allowed on collections sharded by <code>_key</code>.</li><li>In case of the <code>Append</code> save mode, failed jobs cannot be rolled back and the underlying data source may require manual cleanup.</li><li>Speculative execution of tasks only works for idempotent write configurations. See <a href=#write-resiliency class=link>Write Resiliency</a> for more details.</li><li>Speculative execution of tasks can cause concurrent writes to the same documents, resulting in write-write conflicts or lock timeouts</li></ul><h2 id=mapping-configuration>Mapping Configuration <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#mapping-configuration class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Serialization and deserialization of Spark Dataframe Row to and from JSON (or Velocypack) can be customized using the following options:</p><ul><li><code>ignoreNullFields</code>: whether to ignore null fields during serialization, <code>false</code> by default (only supported in Spark 3.x)</li></ul><h2 id=supported-spark-data-types>Supported Spark data types <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#supported-spark-data-types class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The following Spark SQL data types (subtypes of <code>org.apache.spark.sql.types.Filter</code>) are supported for reading, writing and filter pushdown.</p><ul><li><p>Numeric types:</p><ul><li><code>ByteType</code></li><li><code>ShortType</code></li><li><code>IntegerType</code></li><li><code>LongType</code></li><li><code>FloatType</code></li><li><code>DoubleType</code></li></ul></li><li><p>String types:</p><ul><li><code>StringType</code></li></ul></li><li><p>Boolean types:</p><ul><li><code>BooleanType</code></li></ul></li><li><p>Datetime types:</p><ul><li><code>TimestampType</code></li><li><code>DateType</code></li></ul></li><li><p>Complex types:</p><ul><li><code>ArrayType</code></li><li><code>MapType</code> (only with key type <code>StringType</code>)</li><li><code>StructType</code></li></ul></li></ul><h2 id=connect-to-the-arangograph-insights-platform>Connect to the ArangoGraph Insights Platform <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#connect-to-the-arangograph-insights-platform class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>To connect to SSL secured deployments using X.509 Base64 encoded CA certificate (ArangoGraph):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>options</span> <span class=k>=</span> <span class=nc>Map</span><span class=o>(</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;database&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;&lt;dbname&gt;&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;user&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;&lt;username&gt;&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;password&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;&lt;passwd&gt;&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;endpoints&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;&lt;endpoint&gt;:&lt;port&gt;&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;ssl.cert.value&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;&lt;base64 encoded CA certificate&gt;&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;ssl.enabled&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;true&#34;</span><span class=o>,</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;table&#34;</span> <span class=o>-&gt;</span> <span class=s>&#34;&lt;table&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// read
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>myDF</span> <span class=k>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>format</span><span class=o>(</span><span class=s>&#34;com.arangodb.spark&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>options</span><span class=o>(</span><span class=n>options</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>load</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// write
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>import</span> <span class=nn>org.apache.spark.sql.DataFrame</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span><span class=k>:</span> <span class=kt>DataFrame</span> <span class=o>=</span> <span class=c1>//...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>df</span><span class=o>.</span><span class=n>write</span>
</span></span><span class=line><span class=cl>          <span class=o>.</span><span class=n>format</span><span class=o>(</span><span class=s>&#34;com.arangodb.spark&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>          <span class=o>.</span><span class=n>options</span><span class=o>(</span><span class=n>options</span><span class=o>)</span>
</span></span><span class=line><span class=cl>          <span class=o>.</span><span class=n>save</span><span class=o>()</span>
</span></span></code></pre></div><h2 id=current-limitations>Current limitations <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#current-limitations class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><ul><li>For <code>contentType=vpack</code>, implicit deserialization casts don&rsquo;t work well, i.e.
reading a document having a field with a numeric value whereas the related
read schema requires a string value for such a field.</li><li>Dates and timestamps fields are interpreted to be in a UTC time zone.</li><li>In read jobs using <code>stream=true</code> (default), possible AQL warnings are only
logged at the end of each read task (BTS-671).</li><li>Spark SQL <code>DecimalType</code> fields are not supported in write jobs when using <code>contentType=json</code>.</li><li>Spark SQL <code>DecimalType</code> values are written to the database as strings.</li><li><code>byteBatchSize</code> is only considered for <code>contentType=json</code> (DE-226)</li></ul><h2 id=demo>Demo <a href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/#demo class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Check out our <a href=https://github.com/arangodb/arangodb-spark-datasource/tree/main/demo target=_blank rel="noopener noreferrer" class=link>demo</a>&nbsp;<i class="fas fa-external-link-alt"></i>
to learn more about ArangoDB Datasource for Apache Spark.</p><nav class=pagination><span class=prev><a class="nav nav-prev link" href=/3.11/develop/integrations/spring-boot-arangodb/><i class="fas fa-chevron-left fa-fw"></i><p>Spring Boot Starter</p></a></span><span class=next><a class="nav nav-next link" href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/><p>Kafka Connector</p><i class="fas fa-chevron-right fa-fw"></i></a></span></nav></article><div class=toc-container><a class=edit-page aria-label href=https://github.com/arangodb/docs-hugo/edit/main/site/content/3.11/develop/integrations/arangodb-datasource-for-apache-spark.md target=_blank><i class="fab fa-fw fa-github edit-page-icon"></i></a><div class=toc><div class=toc-content><div class=toc-header><p>On this page</p></div><nav id=TableOfContents><div class=level-2><a href=#supported-versions>Supported versions</a></div><div class=level-2><a href=#setup>Setup</a></div><div class=level-2><a href=#general-configuration>General Configuration</a></div><div class=level-3><a href=#ssl>SSL</a></div><div class=level-3><a href=#supported-deployment-topologies>Supported deployment topologies</a></div><div class=level-2><a href=#batch-read>Batch Read</a></div><div class=level-3><a href=#read-configuration>Read Configuration</a></div><div class=level-3><a href=#predicate-and-projection-pushdown>Predicate and Projection Pushdown</a></div><div class=level-3><a href=#read-resiliency>Read Resiliency</a></div><div class=level-2><a href=#batch-write>Batch Write</a></div><div class=level-3><a href=#savemode>SaveMode</a></div><div class=level-3><a href=#write-configuration>Write Configuration</a></div><div class=level-3><a href=#write-resiliency>Write Resiliency</a></div><div class=level-3><a href=#write-requirements>Write requirements</a></div><div class=level-3><a href=#write-limitations>Write Limitations</a></div><div class=level-2><a href=#mapping-configuration>Mapping Configuration</a></div><div class=level-2><a href=#supported-spark-data-types>Supported Spark data types</a></div><div class=level-2><a href=#connect-to-the-arangograph-insights-platform>Connect to the ArangoGraph Insights Platform</a></div><div class=level-2><a href=#current-limitations>Current limitations</a></div><div class=level-2><a href=#demo>Demo</a></div></nav></div></div></div></div></div></section></section></div><button class="back-to-top hidden" onclick=goToTop(event) href=#><i class="fa fa-arrow-up"></i></button><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@docsearch/css@3>
<script src=https://cdn.jsdelivr.net/npm/@docsearch/js@3></script>
<script type=text/javascript>window.setupDocSearch=function(e){if(!window.docsearch)return;docsearch({appId:"OK3ZBQ5982",apiKey:"500c85ccecb335d507fe4449aed12e1d",indexName:"arangodbdocs",insights:!0,container:"#searchbox",debug:!1,maxResultsPerGroup:10,searchParameters:{facetFilters:[`version:${e}`]}})}</script></body></html>