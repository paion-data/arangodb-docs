<!doctype html><html lang=en><head><link href=//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.no-icons.min.css rel=stylesheet><link href=//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css rel=stylesheet><link href=/css/fontawesome-all.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css rel=stylesheet></noscript><link href=/css/featherlight.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/featherlight.min.css rel=stylesheet></noscript><link href=/css/nucleus.css rel=stylesheet><link href=/css/fonts.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fonts.css rel=stylesheet></noscript><link href=/css/theme.css rel=stylesheet><link href=/css/theme-relearn-light.css rel=stylesheet id=variant-style><link href=/css/print.css rel=stylesheet media=print><script src=/js/variant.js?1743775370></script>
<script>var root_url="/",baseUriFull,baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="",window.T_Copied_to_clipboard="",window.T_Copy_link_to_clipboard="",window.T_Link_copied_to_clipboard="",baseUriFull="http://localhost/",window.variants&&variants.init(["relearn-light"])</script><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.119.0"><meta itemprop=description property="description" content="The Kafka connector allows you to export data from Apache Kafka to ArangoDB by writing data from one or more topics in Kafka to a collection in ArangoDB"><meta property="og:url" content="http://localhost/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/"><meta property="og:title" content="Kafka Connect ArangoDB Sink Connector"><meta property="og:type" content="website"><meta property="og:description" content="The Kafka connector allows you to export data from Apache Kafka to ArangoDB by writing data from one or more topics in Kafka to a collection in ArangoDB"><meta name=docsearch:version content="3.11"><title>Kafka Connect ArangoDB Sink Connector | ArangoDB Documentation</title><link href=/images/favicon.png rel=icon type=image/png><script src=/js/jquery.min.js></script>
<script src=/js/clipboard.min.js?1743775370 defer></script>
<script src=/js/featherlight.min.js?1743775370 defer></script>
<script>var versions=[{alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"},{alias:"stable",deprecated:!1,name:"3.12",version:"3.12.4"},{alias:"3.11",deprecated:!1,name:"3.11",version:"3.11.13"},{alias:"3.10",deprecated:!0,name:"3.10",version:"3.10.14"}]</script><script>var develVersion={alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"}</script><script>var stableVersion={alias:"stable",deprecated:!1,name:"3.12",version:"3.12.4"}</script><script src=/js/codeblocks.js?1743775370 defer></script>
<script src=/js/theme.js?1743775370 defer></script></head><body><noscript>You need to enable JavaScript to use the ArangoDB documentation.</noscript><div id=page-wrapper class=page_content_splash style=height:auto;opacity:0><section id=page-main><section class=page-container id=page-container><header id=header style="transition:.5s padding ease-out,.15s" class="zn_header_white header-splash-new nav-down header-splash-wrap header1"><div class=header-block-left><div class=mobile-menu-toggle><button id=sidebar-toggle-navigation onclick=showSidebarHandler()><svg width="1.33em" height="1.33em" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button></div><div class=version-logo-container><div class="logo-container hasinfocard_img arangodb-logo-large"><div class=logo><a href=https://www.arangodb.com/><img src=/images/logo_main.png alt=ArangoDB title></a></div></div><div class=arangodb-logo-small><a href=https://arangodb.com/><img alt="ArangoDB Logo" src=/images/ArangoDB_Logo_White_small.png></a></div></div></div><div class=container-right style=display:hidden></div><div class=search-and-version-container><a href=# class="home-link is-current" aria-label="Go to home page" onclick=goToHomepage(event)></a><div id=searchbox></div><script type=text/javascript>const SCRIPT_SRC="https://unpkg.com/@inkeep/widgets-embed@0.2.290/dist/embed.js";function loadAndInitializeInkeep(){if(document.querySelector(`script[src="${SCRIPT_SRC}]"`))return;const e=document.createElement("script");e.type="module",e.src=SCRIPT_SRC,e.onload=initializeInkeep,document.head.appendChild(e)}function initializeInkeep(){const e=Inkeep({integrationId:"clo4lx6jk0000s601cp21x2ok",apiKey:"13b4e56966a76e86c6ff359cd795ee6a0412f751d75d6383",organizationId:"org_HGBkkzGAa4KeGJGh",organizationDisplayName:"ArangoDB",primaryBrandColor:"#80a54d",stringReplacementRules:[{matchingRule:{ruleType:"Substring",string:"Arangograph"},replaceWith:"ArangoGraph"},{matchingRule:{ruleType:"Substring",string:"Aql"},replaceWith:"AQL"},{matchingRule:{ruleType:"Substring",string:"Arangodb"},replaceWith:"ArangoDB"}],customCardSettings:[{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arango.qubitpi.org"}},searchTabLabel:"Official Docs"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"developer.arangodb.com"}},searchTabLabel:"Developer Hub"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arangodb.com"}},searchTabLabel:"Home"}]}),t=e.embed({componentType:"ChatButton",properties:{stylesheetUrls:["/css/fonts.css"],fixedPositionXOffset:"52px",baseSettings:{theme:{primaryColors:{textColorOnPrimary:"white"},tokens:{fonts:{body:"'Inter'",heading:"'Inter'"},zIndex:{overlay:1e4,modal:11e3,popover:12e3,skipLink:13e3,toast:14e3,tooltip:15e3}}}},aiChatSettings:{botAvatarSrcUrl:"/images/ArangoDB_Logo_White_small.png",quickQuestions:["What can you do with AQL that is not feasible with SQL?","How do I search for objects within arrays?","Where can I deploy my ArangoDB instance?"],getHelpCallToActions:[{icon:{builtIn:"FaSlack"},name:"Slack",url:"https://arangodb-community.slack.com/"}]},searchSettings:{tabSettings:{isAllTabEnabled:!1,alwaysDisplayedTabs:["Official Docs","Developer Hub","Home"]}}}})}loadAndInitializeInkeep()</script><div class=version-selector><select id=arangodb-version onchange=changeVersion()><option value=3.13>3.13</option><option value=3.12>3.12</option><option value=3.11>3.11</option><option value=3.10>3.10</option><option value=3.9>3.9</option><option value=3.8>3.8</option></select></div></div></header><iframe src=/nav.html title=description id=menu-iframe class="menu-iframe active" style=opacity:0></iframe><div class=container-main><div class=row-main><nav id=breadcrumbs><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><meta itemprop=itemListOrder content="Descending"><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="3.11"><a itemprop=item class=link href=/3.11/><span itemprop=name class=breadcrumb-entry>3.11.13</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Develop"><a itemprop=item class=link href=/3.11/develop/><span itemprop=name class=breadcrumb-entry>Develop</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Integrations"><a itemprop=item class=link href=/3.11/develop/integrations/><span itemprop=name class=breadcrumb-entry>Integrations</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Kafka Connector"><a itemprop=item class=link href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/><span itemprop=name class=breadcrumb-entry>Kafka Connector</span></a></li></ol></nav><article class=default><hgroup><h1>Kafka Connect ArangoDB Sink Connector</h1><p class=lead>The Kafka connector allows you to export data from Apache Kafka to ArangoDB by writing data from one or more topics in Kafka to a collection in ArangoDB</p></hgroup><div class="box notices cstyle info"><div class=box-content-container><div class=box-content><i class="fas fa-info-circle"></i><div class=box-text>Check out the <a href=https://github.com/arangodb/kafka-connect-arangodb/tree/main/demo target=_blank rel="noopener noreferrer" class=link>connector demo</a>&nbsp;<i class="fas fa-external-link-alt"></i>
to learn more about the connector.</div></div></div></div><h2 id=supported-versions>Supported versions <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#supported-versions class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>This connector is compatible with:</p><ul><li>Kafka 3.4 and higher versions</li><li>JDK 8 and higher versions</li><li>ArangoDB 3.11.1 and higher versions</li></ul><h2 id=installation>Installation <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#installation class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Download the Jar file from <a href=https://repo1.maven.org/maven2/com/arangodb/kafka-connect-arangodb target=_blank rel="noopener noreferrer" class=link>Maven Central</a>&nbsp;<i class="fas fa-external-link-alt"></i>
and copy it into one of the directories that are listed in the Kafka Connect
worker&rsquo;s <code>plugin.path</code> configuration property. This must be done on each of the
installations where Kafka Connect will be run.</p><p>Once installed, you can then create a connector configuration file with the
connector&rsquo;s settings, and deploy that to a Connect worker.
See the <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/configuration/ class=link>configuration</a> documentation for
the available options.</p><p>For more detailed plugin installation instructions, see the
<a href=https://docs.confluent.io/platform/current/connect/userguide.html#connect-installing-plugins target=_blank rel="noopener noreferrer" class=link>Confluent Documentation</a>&nbsp;<i class="fas fa-external-link-alt"></i>.</p><h2 id=connection-handling>Connection handling <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#connection-handling class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Task connections to the database are evenly distributed across all available
ArangoDB Coordinators. If a connectivity error occur, a connection is
re-established with a different Coordinator.</p><p>Available Coordinators can be periodically monitored by setting
<code>connection.acquireHostList.enabled</code> to <code>true</code> and optionally configuring the
monitoring interval.</p><p>Over time, due to connection failovers, the distribution of task connections
across the Coordinators may become uneven. To address this, connections are
periodically re-balanced across the available Coordinators. The rebalancing
interval can be configured via the <code>connection.rebalance.interval.ms</code> property.
The default is 30 minutes.</p><h2 id=delivery-guarantees>Delivery guarantees <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#delivery-guarantees class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>This connector guarantees that each record in the Kafka topic is delivered at
least once. For example, the same record could be delivered multiple times in
the following scenarios:</p><ul><li>Transient errors in the communication between the connector and the
database system, leading to <a href=#retries class=link>retries</a></li><li>Errors in the communication between the connector and Kafka, preventing to
commit offsets of already written records</li><li>Abrupt termination of connector task</li></ul><p>When restarted, the connector resumes reading from the Kafka topic at an offset
prior to where it stopped. As a result, at least in the cases mentioned above,
some records might get written to ArangoDB more than once. Even if configured
for idempotent writes (e.g. with <code>insert.overwriteMode=replace</code>), writing the
same record multiple times still updates the <code>_rev</code> field of the document.</p><p>Note that in case of retries, <a href=#ordering-guarantees class=link>Ordering Guarantees</a>
are still provided.</p><p>To improve the likelihood that every write survives even in case of a DB-Server
failover, consider configuring the configuration property <code>insert.waitForSync</code>
(default <code>false</code>), which determines whether the write operations are synced
to disk before returning.</p><h2 id=error-handling>Error handling <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#error-handling class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The connector categorizes all the possible errors into two types,
data errors and transient errors.</p><h3 id=data-errors>Data errors <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#data-errors class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>Data errors are unrecoverable and caused by the data being processed.
For example:</p><ul><li>Conversion errors:<ul><li>Illegal key type</li><li>Illegal value type</li></ul></li><li>Server validation errors:<ul><li>Illegal <code>_key</code>, <code>_from</code>, <code>_to</code> values</li><li>JSON schema validation errors</li></ul></li><li>Server constraints violations<ul><li>Unique index violations</li><li>Key conflicts (in case of <code>insert.overwriteMode=conflict</code>)</li></ul></li></ul><p>The configuration property <code>data.errors.tolerance</code> allows you to configure the
behavior for tolerating data errors:</p><ul><li><code>none</code>: data errors result in an immediate connector task failure (default)</li><li><code>all</code>: changes the behavior to skip over records generating data errors.
If DLQ is configured, then the record is reported
(see <a href=#dead-letter-queue class=link>Dead Letter Queue</a>).</li></ul><p>Data errors detection can be further customized via the <code>extra.data.error.nums</code>
configuration property. In addition to the cases listed above, the server errors
reporting <code>errorNums</code> listed by this configuration property are considered
data errors.</p><h3 id=transient-errors>Transient errors <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#transient-errors class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>Transient errors are recoverable and may succeed if retried with some delay
(see <a href=#retries class=link>Retries</a>). If all retries fail, then the connector task fails.</p><p>All errors that are not data errors are considered transient errors.</p><h2 id=retries>Retries <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#retries class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>In case of transient errors, the <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/configuration/#maxretries class=link><code>max.retries</code> configuration property</a>
determines how many times the connector retries.</p><p>The <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/configuration/#retrybackoffms class=link><code>retry.backoff.ms</code> configuration property</a>
allows you to set the time in milliseconds to wait following an error before a
retry attempt is made.</p><p>Data errors are never retried.</p><h2 id=dead-letter-queue>Dead Letter Queue <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#dead-letter-queue class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>This connector supports the Dead Letter Queue (DLQ) functionality.
For information about accessing and using the DLQ,
see <a href=https://docs.confluent.io/platform/current/connect/concepts.html#dead-letter-queue target=_blank rel="noopener noreferrer" class=link>Confluent Platform Dead Letter Queue</a>&nbsp;<i class="fas fa-external-link-alt"></i>.</p><p>Only data errors can be reported to the DLQ. Transient errors, after potential
retries, always make the task fail.</p><p>You can enable DLQ support for data errors by setting <code>data.errors.tolerance=all</code>
and <code>errors.deadletterqueue.topic.name</code>.</p><h2 id=multiple-tasks>Multiple tasks <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#multiple-tasks class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The connector can scale out by running multiple tasks in parallel as specified by the
<a href=https://docs.confluent.io/platform/current/installation/configuration/connect/sink-connect-configs.html#tasks-max target=_blank rel="noopener noreferrer" class=link><code>tasks.max</code></a>&nbsp;<i class="fas fa-external-link-alt"></i>
configuration parameter. In this case, each connector task handles the records
from different Kafka topics partitions.</p><h2 id=data-mapping>Data mapping <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#data-mapping class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The sink connector optionally supports schemas. For example, the Avro converter
that comes with Schema Registry, the JSON converter with schemas enabled, or the
Protobuf converter.</p><p>Kafka record keys and Kafka record value field <code>_key</code>, if present, must be a
primitive type of either:</p><ul><li><code>string</code></li><li><code>integral numeric</code> (integer)</li></ul><p>The record value must be either:</p><ul><li><code>struct</code> (Kafka Connect structured record)</li><li><code>map</code></li><li><code>null</code> (tombstone record)</li></ul><p>Record values are converted to JSON objects using Kafka Connect <code>JsonConverter</code>,
ensuring compatibility with Kafka Connect data types and support to schemas.
If the data in the topic is not of a compatible format, applying an
<a href=https://docs.confluent.io/platform/current/connect/transforms/overview.html target=_blank rel="noopener noreferrer" class=link>SMT</a>&nbsp;<i class="fas fa-external-link-alt"></i>
or implementing a custom converter may be necessary.</p><h2 id=key-handling>Key handling <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#key-handling class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The <code>_key</code> of the documents inserted into ArangoDB is derived in the following way:</p><ol><li>Use the Kafka record value field <code>_key</code> if present and not null, else</li><li>Use the Kafka record key if not null, else</li><li>Use the Kafka record coordinates (<code>topic-partition-offset</code>)</li></ol><h2 id=delete-mode>Delete mode <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#delete-mode class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The connector can delete documents in a database collection when it consumes a
tombstone record, which is a Kafka record that has a non-null key and a null value.
This behavior is disabled by default, meaning that any tombstone records results
in a failure of the connector.</p><p>You can enable deletes with <code>delete.enabled=true</code>.</p><p>Delete operations are idempotent and do not throw errors if the target document
does not exist.</p><p>Enabling delete mode does not affect the <code>insert.overwriteMode</code>.</p><h2 id=write-modes>Write modes <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#write-modes class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The <code>insert.overwriteMode</code> configuration parameter allow you to set the write
behavior in case a document with the same <code>_key</code> already exists:</p><ul><li><code>conflict</code>: the new document value is not written and an exception is thrown (default)</li><li><code>ignore</code>: the new document value is not written</li><li><code>replace</code>: the existing document is overwritten with the new document value</li><li><code>update</code>: the existing document is patched (partially updated) with the new document value</li></ul><h2 id=idempotent-writes>Idempotent writes <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#idempotent-writes class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>All the write modes supported are idempotent, with the exception that the
document revision field (<code>_rev</code>) changes every time a document is written. See
<a href=/3.11/concepts/data-structure/documents/#document-revisions class=link>Document revisions</a>
for more details.</p><p>If there are failures, the Kafka offset used for recovery may not be up-to-date
with what was committed as of the time of the failure, which can lead to
re-processing during recovery. In case of <code>insert.overwriteMode=conflict</code> (default),
this can lead to constraint violations errors if records need to be re-processed.</p><h2 id=ordering-guarantees>Ordering guarantees <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#ordering-guarantees class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Kafka records in the same Kafka topic partition that are mapped to documents with the
same <code>_key</code> (see <a href=#key-handling class=link>Key handling</a>) are written to ArangoDB in the
same order as they are in the Kafka topic partition.</p><p>The order of the writes for records in the same Kafka partition that are mapped
to documents with different <code>_key</code> is not guaranteed.</p><p>The order between writes for records in different Kafka partitions is not guaranteed.</p><p>To guarantee documents in ArangoDB are eventually consistent with the records in
the Kafka topic, it is recommended deriving the document <code>_key</code> from Kafka
record keys and using a key-based partitioner that assigns the same partition to
records with the same key (i.e. Kafka default partitioner).</p><p>Otherwise, in case the document <code>_key</code> is assigned from Kafka record value field
<code>_key</code>, the same could be achieved using a field partitioner on <code>_key</code>.</p><p>When restarted, the connector may resume reading from the Kafka topic at an offset
prior to where it stopped. This can lead to reprocessing of batches containing
multiple Kafka records that are mapped to documents with the same <code>_key</code>.
In such case, it is possible to observe the related document in the database
being temporarily updated to older versions and eventually to newer ones.</p><h2 id=monitoring>Monitoring <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#monitoring class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The Kafka Connect framework exposes basic status information over a REST interface.
Fine-grained metrics, including the number of processed messages and the rate of
processing, are available via JMX. For more information, see
<a href=https://docs.confluent.io/current/connect/managing/monitoring.html target=_blank rel="noopener noreferrer" class=link>Monitoring Kafka Connect and Connectors</a>&nbsp;<i class="fas fa-external-link-alt"></i>
(published by Confluent, but equally applies to a standard Apache Kafka distribution).</p><h2 id=ssl>SSL <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#ssl class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>To connect to ArangoDB using an SSL connection, you must set the <code>ssl.enabled</code>
configuration property to <code>true</code>.</p><h3 id=certificate-from-file>Certificate from file <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#certificate-from-file class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>The connector can load the trust store to be used from file. The following
configuration properties can be used:</p><ul><li><code>ssl.truststore.location</code>: the location of the trust store file</li><li><code>ssl.truststore.password</code>: the password for the trust store file</li></ul><p>Note that the trust store file path needs to be accessible at the same given
location from all Kafka Connect workers.</p><h3 id=certificate-from-configuration-property-value>Certificate from configuration property value <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#certificate-from-configuration-property-value class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>The connector can accept the SSL certificate value from configuration property
encoded as Base64 string. The following configuration properties can be used:</p><ul><li><code>ssl.cert.value</code>: Base64-encoded SSL certificate</li></ul><p>See <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/configuration/#ssl class=link>SSL configuration</a> for further options.</p><h2 id=limitations>Limitations <a href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/#limitations class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><ul><li>Record values are required to be object-like structures (DE-644)</li><li>Auto-creation of ArangoDB collection is not supported (DE-653)</li><li>Batch writes are not guaranteed to be executed serially (FRB-300)</li><li>Batch writes may succeed for some documents while failing for others (FRB-300)
This has two important consequences:<ul><li>Transient errors might be retried and succeed at a later point</li><li>Data errors might be asynchronously reported to the DLQ</li></ul></li></ul><nav class=pagination><span class=prev><a class="nav nav-prev link" href=/3.11/develop/integrations/arangodb-datasource-for-apache-spark/><i class="fas fa-chevron-left fa-fw"></i><p>Datasource for Apache Spark</p></a></span><span class=next><a class="nav nav-next link" href=/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/configuration/><p>Configuration</p><i class="fas fa-chevron-right fa-fw"></i></a></span></nav></article><div class=toc-container><a class=edit-page aria-label href=https://github.com/arangodb/docs-hugo/edit/main/site/content/3.11/develop/integrations/kafka-connect-arangodb-sink-connector/_index.md target=_blank><i class="fab fa-fw fa-github edit-page-icon"></i></a><div class=toc><div class=toc-content><div class=toc-header><p>On this page</p></div><nav id=TableOfContents><div class=level-2><a href=#supported-versions>Supported versions</a></div><div class=level-2><a href=#installation>Installation</a></div><div class=level-2><a href=#connection-handling>Connection handling</a></div><div class=level-2><a href=#delivery-guarantees>Delivery guarantees</a></div><div class=level-2><a href=#error-handling>Error handling</a></div><div class=level-3><a href=#data-errors>Data errors</a></div><div class=level-3><a href=#transient-errors>Transient errors</a></div><div class=level-2><a href=#retries>Retries</a></div><div class=level-2><a href=#dead-letter-queue>Dead Letter Queue</a></div><div class=level-2><a href=#multiple-tasks>Multiple tasks</a></div><div class=level-2><a href=#data-mapping>Data mapping</a></div><div class=level-2><a href=#key-handling>Key handling</a></div><div class=level-2><a href=#delete-mode>Delete mode</a></div><div class=level-2><a href=#write-modes>Write modes</a></div><div class=level-2><a href=#idempotent-writes>Idempotent writes</a></div><div class=level-2><a href=#ordering-guarantees>Ordering guarantees</a></div><div class=level-2><a href=#monitoring>Monitoring</a></div><div class=level-2><a href=#ssl>SSL</a></div><div class=level-3><a href=#certificate-from-file>Certificate from file</a></div><div class=level-3><a href=#certificate-from-configuration-property-value>Certificate from configuration property value</a></div><div class=level-2><a href=#limitations>Limitations</a></div></nav></div></div></div></div></div></section></section></div><button class="back-to-top hidden" onclick=goToTop(event) href=#><i class="fa fa-arrow-up"></i></button><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@docsearch/css@3>
<script src=https://cdn.jsdelivr.net/npm/@docsearch/js@3></script>
<script type=text/javascript>window.setupDocSearch=function(e){if(!window.docsearch)return;docsearch({appId:"OK3ZBQ5982",apiKey:"500c85ccecb335d507fe4449aed12e1d",indexName:"arangodbdocs",insights:!0,container:"#searchbox",debug:!1,maxResultsPerGroup:10,searchParameters:{facetFilters:[`version:${e}`]}})}</script></body></html>